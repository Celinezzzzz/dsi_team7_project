{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index       Date        Open        High         Low       Close  \\\n",
      "0   NYA 1965-12-31  528.690002  528.690002  528.690002  528.690002   \n",
      "1   NYA 1966-01-03  527.210022  527.210022  527.210022  527.210022   \n",
      "2   NYA 1966-01-04  527.840027  527.840027  527.840027  527.840027   \n",
      "3   NYA 1966-01-05  531.119995  531.119995  531.119995  531.119995   \n",
      "4   NYA 1966-01-06  532.070007  532.070007  532.070007  532.070007   \n",
      "\n",
      "    Adj Close  Volume    CloseUSD  \n",
      "0  528.690002     0.0  449.386502  \n",
      "1  527.210022     0.0  448.128519  \n",
      "2  527.840027     0.0  448.664023  \n",
      "3  531.119995     0.0  451.451996  \n",
      "4  532.070007     0.0  452.259506  \n",
      "Number of rows: 106899\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandasql as psql\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../data/raw/indexData.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM df\n",
    "WHERE Open IS NOT NULL\n",
    "AND Close IS NOT NULL\n",
    "AND Volume IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "df_cleaned = psql.sqldf(query, locals())\n",
    "\n",
    "\n",
    "# 1. Drop rows with any missing (NaN) values\n",
    "#df_cleaned = df.dropna()\n",
    "\n",
    "# Define thresholds for outliers based on the 1st and 99th percentiles for each feature\n",
    "\n",
    "volume_threshold = (df_cleaned['Volume'].quantile(0.01), df_cleaned['Volume'].quantile(0.99))\n",
    "open_threshold = (df_cleaned['Open'].quantile(0.01), df_cleaned['Open'].quantile(0.99))\n",
    "high_threshold = (df_cleaned['High'].quantile(0.01), df_cleaned['High'].quantile(0.99))\n",
    "low_threshold = (df_cleaned['Low'].quantile(0.01), df_cleaned['Low'].quantile(0.99))\n",
    "close_threshold = (df_cleaned['Close'].quantile(0.01), df_cleaned['Close'].quantile(0.99))\n",
    "adj_close_threshold = (df_cleaned['Adj Close'].quantile(0.01), df_cleaned['Adj Close'].quantile(0.99))\n",
    "\n",
    "# Remove rows where 'Volume', 'Open', 'High', 'Low', 'Close', or 'Adj Close' fall outside of these thresholds\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned['Volume'] >= volume_threshold[0]) & (df_cleaned['Volume'] <= volume_threshold[1]) &\n",
    "    (df_cleaned['Open'] >= open_threshold[0]) & (df_cleaned['Open'] <= open_threshold[1]) &\n",
    "    (df_cleaned['High'] >= high_threshold[0]) & (df_cleaned['High'] <= high_threshold[1]) &\n",
    "    (df_cleaned['Low'] >= low_threshold[0]) & (df_cleaned['Low'] <= low_threshold[1]) &\n",
    "    (df_cleaned['Close'] >= close_threshold[0]) & (df_cleaned['Close'] <= close_threshold[1]) &\n",
    "    (df_cleaned['Adj Close'] >= adj_close_threshold[0]) & (df_cleaned['Adj Close'] <= adj_close_threshold[1])\n",
    "]\n",
    "\n",
    "# 3. Remove Duplicates (if any)\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# 4. (Optional) Filter rows based on a specific date range (assuming there's a 'Date' column)\n",
    "# Convert 'Date' column to datetime if needed\n",
    "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'])\n",
    "\n",
    "# 5. Example of creating a new feature (similar to `CloseUSD`)\n",
    "# Let's assume we are converting 'Close' price to USD using a fixed exchange rate\n",
    "usd_conversion_rate = 0.85  # Example conversion rate\n",
    "df_cleaned['CloseUSD'] = df_cleaned['Close'] * usd_conversion_rate\n",
    "\n",
    "# Show the result\n",
    "print(df_cleaned.head())\n",
    "num_rows = len(df_cleaned)\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "# Save the cleaned dataset if needed\n",
    "df_cleaned.to_csv('../data/processed/indexData_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.45      0.46     10042\n",
      "           1       0.53      0.56      0.54     11338\n",
      "\n",
      "    accuracy                           0.50     21380\n",
      "   macro avg       0.50      0.50      0.50     21380\n",
      "weighted avg       0.50      0.50      0.50     21380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load cleaned data\n",
    "file_path = '../data/processed/indexData_processed.csv'\n",
    "df_cleaned = pd.read_csv(file_path)\n",
    "\n",
    "# Feature Engineering: Create target variable (1 for increase, 0 for decrease)\n",
    "df_cleaned['Next_Close'] = df_cleaned['Close'].shift(-1)  # Shift the 'Close' column to compare with next day\n",
    "df_cleaned['Target'] = (df_cleaned['Next_Close'] > df_cleaned['Close']).astype(int)\n",
    "\n",
    "# Drop the last row as it will have a NaN 'Next_Close'\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Features (X) and target (y)\n",
    "features = ['Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned['Target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5293732460243218\n",
      "Random Forest Accuracy: 0.5078110383536015\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     10062\n",
      "           1       0.53      1.00      0.69     11318\n",
      "\n",
      "    accuracy                           0.53     21380\n",
      "   macro avg       0.26      0.50      0.35     21380\n",
      "weighted avg       0.28      0.53      0.37     21380\n",
      "\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.45      0.46     10062\n",
      "           1       0.53      0.56      0.55     11318\n",
      "\n",
      "    accuracy                           0.51     21380\n",
      "   macro avg       0.50      0.50      0.50     21380\n",
      "weighted avg       0.51      0.51      0.51     21380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Besher\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Users\\Besher\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Users\\Besher\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/processed/indexData_processed.csv')\n",
    "#data = pd.read_csv('../data/raw/indexProcessed.csv')\n",
    "\n",
    "# Create a target variable: Price_Change (1 = Increase, 0 = Decrease)\n",
    "data['Price_Change'] = (data['Close'].shift(-1) > data['Close']).astype(int)\n",
    "\n",
    "# Drop the last row since it won't have a target value\n",
    "data = data[:-1]\n",
    "\n",
    "# Features for the model (without CloseUSD)\n",
    "features = ['Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
    "X = data[features]\n",
    "y = data['Price_Change']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nClassification Report for Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "print(\"\\nClassification Report for Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/indexData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 1. Feature Engineering - Create new features\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_Return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdj Close\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpct_change()\n\u001b[0;32m     16\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5_day_MA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     17\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10_day_MA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df_cleaned = pd.read_csv('../data/processed/indexData_processed.csv')\n",
    "\n",
    "# 1. Feature Engineering - Create new features\n",
    "df_cleaned['Price_Return'] = df_cleaned['Adj Close'].pct_change()\n",
    "df_cleaned['5_day_MA'] = df_cleaned['Adj Close'].rolling(window=5).mean()\n",
    "df_cleaned['10_day_MA'] = df_cleaned['Adj Close'].rolling(window=10).mean()\n",
    "\n",
    "# Drop rows with missing values created by pct_change and rolling mean\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Target: Whether 'Adj Close' will increase or decrease\n",
    "df_cleaned['Target'] = (df_cleaned['Adj Close'].shift(-1) > df_cleaned['Adj Close']).astype(int)\n",
    "\n",
    "# 2. Prepare data for modeling\n",
    "X = df_cleaned[['Open', 'High', 'Low', 'Adj Close', 'Volume', 'Price_Return', '5_day_MA', '10_day_MA']]\n",
    "y = df_cleaned['Target']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Logistic Regression with Class Weight\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# 4. Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "print(\"Gradient Boosting Classifier:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# 5. Support Vector Classifier\n",
    "svc = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc.predict(X_test_scaled)\n",
    "print(\"Support Vector Classifier:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "# 6. Hyperparameter Tuning using GridSearchCV for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "y_pred_best_log_reg = best_log_reg.predict(X_test_scaled)\n",
    "print(\"Best Logistic Regression from Grid Search:\")\n",
    "print(classification_report(y_test, y_pred_best_log_reg))\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    price_diff        ma_5       ma_10       ma_30  volatility  pct_return  \\\n",
      "29         0.0  537.566003  534.946002  534.558667         0.0   -0.002741   \n",
      "30         0.0  538.138001  535.210004  534.879333         0.0   -0.000186   \n",
      "31         0.0  538.244006  535.654004  535.231667         0.0   -0.000985   \n",
      "32         0.0  537.968005  536.288007  535.485333         0.0   -0.004333   \n",
      "33         0.0  537.059998  536.679004  535.626333         0.0   -0.000187   \n",
      "\n",
      "    price_direction  \n",
      "29                0  \n",
      "30                0  \n",
      "31                0  \n",
      "32                0  \n",
      "33                0  \n",
      "Logistic Regression Accuracy: 0.5289604192008983\n",
      "\n",
      "Random Forest Accuracy: 0.5140357443623094\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.01      0.01     10076\n",
      "           1       0.53      1.00      0.69     11298\n",
      "\n",
      "    accuracy                           0.53     21374\n",
      "   macro avg       0.53      0.50      0.35     21374\n",
      "weighted avg       0.53      0.53      0.37     21374\n",
      "\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.44      0.46     10076\n",
      "           1       0.54      0.58      0.56     11298\n",
      "\n",
      "    accuracy                           0.51     21374\n",
      "   macro avg       0.51      0.51      0.51     21374\n",
      "weighted avg       0.51      0.51      0.51     21374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../data/processed/indexData_processed.csv')\n",
    "\n",
    "# Create daily price difference\n",
    "data['price_diff'] = data['Close'] - data['Open']\n",
    "\n",
    "# Calculate moving averages for closing prices (5-day, 10-day, 30-day)\n",
    "data['ma_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['ma_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['ma_30'] = data['Close'].rolling(window=30).mean()\n",
    "\n",
    "# Calculate daily volatility using high and low prices\n",
    "data['volatility'] = data['High'] - data['Low']\n",
    "\n",
    "# Calculate percentage returns\n",
    "data['pct_return'] = data['Close'].pct_change()\n",
    "\n",
    "# Drop the first row due to NaN values created by pct_change\n",
    "data = data.dropna()\n",
    "\n",
    "# Create a target variable: 1 if next day's closing price increases, 0 otherwise\n",
    "data['price_direction'] = (data['Close'].shift(-1) > data['Close']).astype(int)\n",
    "\n",
    "# Drop the last row since it won't have a target value\n",
    "data = data[:-1]\n",
    "\n",
    "# Display the new features\n",
    "print(data[['price_diff', 'ma_5', 'ma_10', 'ma_30', 'volatility', 'pct_return', 'price_direction']].head())\n",
    "\n",
    "\n",
    "output_file_path = '../data/processed/indexData_processed2.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the modified data with new features\n",
    "data = pd.read_csv('../data/processed/indexData_processed2.csv')\n",
    "\n",
    "# Prepare the features and target variable\n",
    "features = ['price_diff', 'ma_5', 'ma_10', 'ma_30', 'volatility', 'pct_return']\n",
    "X = data[features]\n",
    "y = data['price_direction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "log_reg_report = classification_report(y_test, y_pred_log)\n",
    "rf_report = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Output the results\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "print(\"\\nRandom Forest Accuracy:\", rf_accuracy)\n",
    "print(\"\\nClassification Report for Logistic Regression:\\n\", log_reg_report)\n",
    "print(\"\\nClassification Report for Random Forest:\\n\", rf_report)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
